import json
import copy
import re
import random
from typing import Dict, List, Sequence, Union, Any
import torch
from torch.utils.data import Dataset
import transformers
from transformers import PreTrainedTokenizer
from transformers.trainer_pt_utils import LabelSmoother
import torch.distributed as dist

from hidden_state_loader import HiddenStateLoader

from fastchat.conversation import SeparatorStyle
from fastchat.model.model_adapter import get_conversation_template, get_model_adapter

IGNORE_TOKEN_ID = LabelSmoother.ignore_index

local_rank = None

def rank0_print(*args):
    """Print only on rank 0 in distributed training"""
    if local_rank is None or local_rank == 0:
        print(*args)

def debug_mask_for_conversation(
    conversation: str,
    target: torch.Tensor,
    tokenizer: transformers.PreTrainedTokenizer,
    ignore_token_id: int = -100,
    max_segments: int = 100,
    max_chars_per_segment: int = 8000,
):
    """
    Visualize which tokens in a sample are ignored (not contributing to loss)
    and which tokens are supervised.
    Tokens are grouped into continuous segments by IGNORE / LABEL status and printed segment by segment.

    Args:
      - conversation: original prompt string (output of conv.get_prompt())
      - target: corresponding label tensor (1D or 2D, length = tokenizer.model_max_length)
      - tokenizer: the same tokenizer
      - ignore_token_id: usually -100
    """

    # ---- 1. Flatten target to 1D ----
    if target.dim() == 2:
        target = target[0]
    target = target.detach().cpu()

    pad_id = tokenizer.pad_token_id

    # ---- 2. Re-tokenize conversation in the same way as training ----
    tokenized = tokenizer(
        conversation,
        return_tensors="pt",
        padding="max_length",
        max_length=tokenizer.model_max_length,
        truncation=True,
    )
    input_ids = tokenized.input_ids[0]  # [seq_len]
    input_ids = input_ids.detach().cpu()

    # ---- 3. Only inspect non-pad tokens ----
    total_len = int(input_ids.ne(pad_id).sum())
    print("=" * 120)
    print(f"[DEBUG] total non-pad length = {total_len}")
    print(f"[DEBUG] first 32 token_ids: {input_ids[:32].tolist()}")
    print(f"[DEBUG] first 32 target   : {target[:32].tolist()}")
    print("=" * 120)

    # ---- 4. Segment by IGNORE / LABEL ----
    segments = []  # list of (status, start_idx, end_idx)
    cur_status = None
    cur_start = 0

    for idx in range(total_len):
        tid = input_ids[idx].item()
        lab = target[idx].item()

        # Pad tokens are treated as IGNORE
        if tid == pad_id:
            status = "PAD"
        else:
            status = "IGN" if lab == ignore_token_id else "LAB"

        if cur_status is None:
            cur_status = status
            cur_start = idx
        elif status != cur_status:
            segments.append((cur_status, cur_start, idx))
            cur_status = status
            cur_start = idx

    # Append the last segment
    if cur_status is not None:
        segments.append((cur_status, cur_start, total_len))

    # ---- 5. Print text for each segment ----
    print("[DEBUG] Segments (grouped by IGN/LAB):")
    for seg_idx, (status, s, e) in enumerate(segments):
        if seg_idx >= max_segments:
            print(f"... (segments truncated at {max_segments})")
            break

        seg_ids = input_ids[s:e].tolist()
        # Do not skip special tokens so that <|eot_id|>, <|im_end|>, etc. are visible
        text = tokenizer.decode(seg_ids, skip_special_tokens=False)

        # Preview only, avoid overly long segments
        preview = text.replace("\n", "\\n")
        if len(preview) > max_chars_per_segment:
            preview = preview[:max_chars_per_segment] + "..."

        print(f"[{seg_idx:02d}] [{status}] tokens[{s}:{e}] (len={e-s}): {repr(preview)}")

    print("=" * 120)


def preprocess_with_position_tracking(
        sources,
        tokenizer: transformers.PreTrainedTokenizer,
        model_path: str,
) -> Dict:
    """Preprocess data by tokenizing and tracking first human positions."""

    def decode_targets(
        targets: torch.Tensor,
        tokenizer,
        ignore_token_id: int = -100,
        pad_token_id: int = None,
        skip_special_tokens: bool = False,
    ) -> str:
        """
        Decode a single sample's target (label tensor) into a string.
        ignore_token_id (default -100) and pad_token_id are skipped.
        """
        if targets.dim() == 2:
            targets = targets[0]
        targets = targets.detach().cpu().clone()

        if pad_token_id is None:
            pad_token_id = tokenizer.pad_token_id

        valid_ids = [
            tid.item()
            for tid in targets
            if tid != ignore_token_id and tid != pad_token_id
        ]

        if not valid_ids:
            return ""

        decoded = tokenizer.decode(valid_ids, skip_special_tokens=skip_special_tokens)
        return decoded

    # ------- 1. Get base conversation template and inject system prompt -------

    base_conv = get_model_adapter(model_path).get_default_conv_template(model_path)

    # If the template has no system message, add one
    if getattr(base_conv, "system_message", None) in (None, ""):
        base_conv.set_system_message("You are a helpful assistant.")

    roles = {"human": base_conv.roles[0], "gpt": base_conv.roles[1]}

    # Apply prompt templates
    conversations: List[str] = []
    human_end_markers: List[str] = []  # Whether each sample has <FIRST_HUMAN_END>

    # ------- 2. Build each conversation string and mark the first human message -------

    for i, source in enumerate(sources):
        # Use a fresh conv per sample to avoid message residue
        conv = copy.deepcopy(base_conv)

        if roles[source[0]["from"]] != conv.roles[0]:
            # Skip the first one if it is not from human
            source = source[1:]

        first_human_content = None

        for j, sentence in enumerate(source):
            role = roles[sentence["from"]]
            assert role == conv.roles[j % 2], f"{i}"

            if sentence["from"] == "human" and first_human_content is None:
                first_human_content = sentence["value"]
                # Append marker at the end of the first human message
                marked_content = sentence["value"] + "<FIRST_HUMAN_END>"
                conv.append_message(role, marked_content)
            else:
                conv.append_message(role, sentence["value"])

        conversations.append(conv.get_prompt())
        human_end_markers.append("<FIRST_HUMAN_END>" if first_human_content else None)

    # ------- 3. Tokenize conversations -------

    tokenized = tokenizer(
        conversations,
        return_tensors="pt",
        padding="max_length",
        max_length=tokenizer.model_max_length,
        truncation=True,
    )

    input_ids = tokenized.input_ids
    targets = input_ids.clone()

    # ------- 4. Locate <FIRST_HUMAN_END> positions in each sample -------

    human_end_positions = []
    marker_token_ids = tokenizer("<FIRST_HUMAN_END>", add_special_tokens=False).input_ids
    marker_token_ids_t = torch.tensor(marker_token_ids, dtype=input_ids.dtype)

    for batch_idx in range(input_ids.shape[0]):
        if human_end_markers[batch_idx]:
            position = -1
            row = input_ids[batch_idx]
            for i_pos in range(len(row) - len(marker_token_ids) + 1):
                if torch.all(row[i_pos:i_pos + len(marker_token_ids)] == marker_token_ids_t):
                    position = i_pos  # record marker start position
                    # Replace marker with pad tokens (model should not see it)
                    row[i_pos:i_pos + len(marker_token_ids)] = tokenizer.pad_token_id
                    break
            human_end_positions.append(position)
        else:
            human_end_positions.append(-1)

    human_end_positions = torch.tensor(human_end_positions, dtype=torch.long)

    # ------- 5. Compute label masks according to sep_style -------

    sep_style = base_conv.sep_style

    # --------- 5.1 Qwen2.5 / ChatML style ---------
    if sep_style == SeparatorStyle.CHATML:
        # Handle Qwen2.5 style
        sep2 = "<|im_end|>\n"
        sep = base_conv.roles[1] + "\n"
        sep_len = len(tokenizer(sep, add_special_tokens=False).input_ids)
        sep2_len = len(tokenizer(sep2, add_special_tokens=False).input_ids)

        for conversation, target in zip(conversations, targets):
            total_len = int(target.ne(tokenizer.pad_token_id).sum())

            turns = conversation.split(sep2)
            cur_len = 1  # skip BOS
            for i_turn, turn in enumerate(turns):
                if turn == "":
                    break
                if "<|im_start|>system\nYou are a helpful assistant." == turn:
                    sys_len = len(
                        tokenizer("system\nYou are a helpful assistant.", add_special_tokens=False).input_ids
                    )
                    target[cur_len: cur_len + sys_len] = IGNORE_TOKEN_ID
                    cur_len += sys_len
                elif i_turn % 2 == 1:
                    # human / instruction
                    instruction_len = len(tokenizer(turn, add_special_tokens=False).input_ids)
                    target[cur_len + 1: cur_len + instruction_len] = IGNORE_TOKEN_ID
                    cur_len += instruction_len
                else:
                    # assistant
                    turn_len = len(tokenizer(turn, add_special_tokens=False).input_ids)
                    target[cur_len + 1: cur_len + sep_len] = IGNORE_TOKEN_ID
                    cur_len += turn_len

                cur_len += sep2_len

            target[cur_len:] = IGNORE_TOKEN_ID

            if cur_len < tokenizer.model_max_length:
                if cur_len != total_len:
                    target[:] = IGNORE_TOKEN_ID
                    print(
                        f"WARNING: tokenization mismatch: {cur_len} vs. {total_len}."
                        f" #turn = {len(turns) - 1}. (ignored)"
                    )

    # --------- 5.2 LLaMA3 style ---------
    elif sep_style == SeparatorStyle.LLAMA3:  # for llama3
        sep2 = "<|eot_id|>"
        sep2_len = len(tokenizer(sep2, add_special_tokens=False).input_ids)

        # Match header: <|start_header_id|>role<|end_header_id|>
        role_pattern = re.compile(
            r"<\|start_header_id\|>(.*?)<\|end_header_id\|>",
            re.DOTALL
        )

        for conversation, target in zip(conversations, targets):
            total_len = int(target.ne(tokenizer.pad_token_id).sum())

            # Each turn = header + content, excluding <|eot_id|>
            turns = conversation.split(sep2)

            # The first token (<|begin_of_text|>) is usually ignored
            cur_len = 0

            for turn in turns:
                if turn == "":
                    break

                turn_ids = tokenizer(turn, add_special_tokens=False).input_ids
                turn_len = len(turn_ids)

                # Parse role
                m = role_pattern.search(turn)
                role = m.group(1).strip() if m else None  # "system"/"user"/"assistant"/None

                if role is None:
                    # Unknown segment, ignore entire turn
                    target[cur_len: cur_len + turn_len] = IGNORE_TOKEN_ID
                    cur_len += turn_len

                elif role in ("system", "user"):
                    # system & user: ignore entire turn
                    target[cur_len: cur_len + turn_len] = IGNORE_TOKEN_ID
                    cur_len += turn_len

                elif role == "assistant":
                    # assistant: ignore header, keep content as labels
                    header_text = m.group(0)
                    header_len = len(
                        tokenizer(header_text, add_special_tokens=False).input_ids
                    )
                    target[cur_len: cur_len + header_len] = IGNORE_TOKEN_ID
                    cur_len += turn_len

                else:
                    # Other unexpected roles: ignore entire turn
                    target[cur_len: cur_len + turn_len] = IGNORE_TOKEN_ID
                    cur_len += turn_len

                # Skip and handle <|eot_id|>
                if cur_len + sep2_len > tokenizer.model_max_length:
                    break
                if role in ("system", "user", None):
                    target[cur_len: cur_len + sep2_len] = IGNORE_TOKEN_ID
                elif role == "assistant":
                    pass
                cur_len += sep2_len

            # Ignore the rest
            target[cur_len:] = IGNORE_TOKEN_ID

            if cur_len < tokenizer.model_max_length:
                if cur_len != total_len:
                    target[:] = IGNORE_TOKEN_ID
                    print(
                        f"WARNING: tokenization mismatch (llama3): {cur_len} vs. {total_len}."
                        f" #turn = {len(turns) - 1}. (ignored)"
                    )

    debug_mask_for_conversation(
            conversation=conversation,
            target=target,
            tokenizer=tokenizer,
            ignore_token_id=IGNORE_TOKEN_ID,
        )

    # Other sep_style branches can be added if needed

    return dict(
        input_ids=input_ids,
        labels=targets,
        attention_mask=input_ids.ne(tokenizer.pad_token_id),
        human_end_positions=human_end_positions,
    )

class SupervisedDataset(Dataset):
    """Dataset for supervised fine-tuning with position tracking (eager loading - not recommended for large datasets)."""

    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer, model_path: str = None):
        super(SupervisedDataset, self).__init__()

        rank0_print("[WARNING] Using SupervisedDataset with eager loading. For large datasets, use LazySupervisedDataset instead.")
        rank0_print("Formatting inputs with position tracking...")
        sources = [example["conversations"] for example in raw_data]

        # Only pass conversations for preprocessing, without plan or extra data
        data_dict = preprocess_with_position_tracking(sources, tokenizer, model_path)

        self.input_ids = data_dict["input_ids"]
        self.labels = data_dict["labels"]
        self.attention_mask = data_dict["attention_mask"]
        self.human_end_positions = data_dict["human_end_positions"]
        self.tokenizer = tokenizer

        # Directly extract from raw_data to keep index alignment
        self.plans = []
        self.hidden_states = []

        for i, example in enumerate(raw_data):
            # Plan data
            plan = example.get('plan', '')
            plan_ids = self.tokenizer(plan, add_special_tokens=False).input_ids
            self.plans.append(plan_ids)

            # Hidden state data
            if 'hidden_state' in example:
                hidden_state = example['hidden_state']
                if isinstance(hidden_state, torch.Tensor):
                    self.hidden_states.append(hidden_state)
                else:
                    self.hidden_states.append(torch.tensor(hidden_state, dtype=torch.float32))
            else:
                self.hidden_states.append(None)

        # Verify data consistency
        assert len(self.input_ids) == len(self.plans) == len(self.hidden_states), \
            f"Data length mismatch: input_ids={len(self.input_ids)}, plans={len(self.plans)}, hidden_states={len(self.hidden_states)}"

        rank0_print(f"Dataset loaded: {len(self.input_ids)} examples")

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, i) -> Dict[str, torch.Tensor]:
        result = dict(
            input_ids=self.input_ids[i],
            labels=self.labels[i],
            attention_mask=self.attention_mask[i],
            human_end_positions=self.human_end_positions[i],
            plan=self.plans[i],  # ensure index alignment
        )

        if self.hidden_states[i] is not None:
            result['prepended_hidden_states'] = self.hidden_states[i]

        return result


class LazySupervisedDataset(Dataset):
    """Dataset for supervised fine-tuning with lazy preprocessing and position tracking.
    
    This dataset does NOT cache processed data to avoid memory overflow.
    Data is processed on-the-fly for each access.
    
    Note: In multi-worker DataLoader, each worker will have its own copy of this dataset
    and the HiddenStateLoader. To avoid memory issues, we defer the creation of
    HiddenStateLoader to the worker processes.
    """

    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer, model_path: str = None, 
                 hidden_state_loader: HiddenStateLoader = None, prepended_length: int = 800,
                 hidden_data_path: str = None):
        super(LazySupervisedDataset, self).__init__()

        rank0_print("Initializing LazySupervisedDataset in lazy mode (no preprocessing, no caching)")
        self.tokenizer = tokenizer
        self.raw_data = raw_data
        self.model_path = model_path
        self.hidden_state_loader = hidden_state_loader
        self.prepended_length = prepended_length
        self.hidden_data_path = hidden_data_path
        
        # Worker-local loader (will be initialized in worker processes)
        self._worker_loader = None
        
        rank0_print(f"Dataset initialized with {len(self.raw_data)} examples")

    def _get_loader(self):
        """Get the HiddenStateLoader.
        
        The loader is created once in the main process and shared to workers
        via fork() copy-on-write. No per-worker loading needed.
        """
        if self._worker_loader is None:
            if self.hidden_state_loader is not None:
                self._worker_loader = self.hidden_state_loader
            elif self.hidden_data_path is not None:
                # Fallback: create loader if not provided (should not happen normally)
                rank0_print(f"Warning: Creating HiddenStateLoader in worker (fallback)")
                self._worker_loader = HiddenStateLoader(self.hidden_data_path)
        return self._worker_loader

    def __len__(self):
        return len(self.raw_data)

    def __getitem__(self, i) -> Dict[str, torch.Tensor]:
        # Process conversation data on-the-fly (no caching)
        ret = preprocess_with_position_tracking(
            [self.raw_data[i]["conversations"]],
            self.tokenizer,
            self.model_path
        )

        # Extract plan from raw data
        plan = self.raw_data[i].get('plan', '')
        plan_ids = self.tokenizer(plan, add_special_tokens=False).input_ids

        # Build return dict
        result = dict(
            input_ids=ret["input_ids"][0],
            labels=ret["labels"][0],
            attention_mask=ret["attention_mask"][0],
            human_end_positions=ret["human_end_positions"][0],
            plan=plan_ids,
        )

        # Load hidden state on-the-fly from raw data or loader
        if 'hidden_state' in self.raw_data[i]:
            hidden_state = self.raw_data[i]['hidden_state']
            if isinstance(hidden_state, torch.Tensor):
                result['prepended_hidden_states'] = hidden_state
            else:
                result['prepended_hidden_states'] = torch.tensor(hidden_state, dtype=torch.float32)
        elif 'id' in self.raw_data[i]:
            # Load hidden state from loader if available
            loader = self._get_loader()
            if loader is not None:
                try:
                    hidden_state, _ = loader.get_hidden_state_and_plan(self.raw_data[i]['id'])
                    hidden_length = hidden_state.shape[0]
                    if hidden_length >= self.prepended_length:
                        hidden_state = hidden_state[:self.prepended_length, :]
                    result['prepended_hidden_states'] = hidden_state
                except Exception as e:
                    # If loading fails, skip hidden state for this example
                    rank0_print(f"Warning: Failed to load hidden state for example {i}: {e}")

        return result


class DataCollatorForSupervisedDataset(object):
    """Collate examples for supervised fine-tuning with plan data."""

    def __init__(self, tokenizer: transformers.PreTrainedTokenizer):
        self.tokenizer = tokenizer

    def __call__(self, instances: Sequence[Dict]) -> Dict[str, Union[torch.Tensor, List]]:
        input_ids, labels = tuple([instance[key] for instance in instances] for key in ("input_ids", "labels"))
        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id
        )
        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_TOKEN_ID)

        attention_mask = input_ids.ne(self.tokenizer.pad_token_id)

        # Process human_end_positions
        human_end_positions = [instance.get("human_end_positions", -1) for instance in instances]
        human_end_positions = torch.tensor(human_end_positions)

        # Process prepended_hidden_states
        prepended_hidden_states = []
        for instance in instances:
            if "prepended_hidden_states" in instance:
                prepended_hidden_states.append(instance["prepended_hidden_states"])
            else:
                prepended_hidden_states.append(None)

        # Process plan data
        plans = [instance.get("plan", "") for instance in instances]

        result = dict(
            input_ids=input_ids,
            labels=labels,
            attention_mask=attention_mask,
            human_end_positions=human_end_positions,
            plans=plans,
        )

        if prepended_hidden_states is not None:
            result["prepended_hidden_states"] = prepended_hidden_states

        return result


def make_supervised_data_module(
        tokenizer: transformers.PreTrainedTokenizer,
        data_args,
        model_path: str = None,
        use_position_tracking: bool = True,
        prepended_length=800
) -> Dict:
    """Create dataset and collator for supervised fine-tuning.
    
    This function now supports lazy loading to avoid memory overflow.
    When lazy_preprocess is True, data is processed on-the-fly without caching.
    
    Memory optimization strategy:
    - In lazy mode: HiddenStateLoader is NOT created in main process to avoid
      copying large data to worker processes. Instead, workers create their own loaders.
    - In non-lazy mode: All data is loaded upfront (not recommended for large datasets).
    """
    
    # NOTE: Do NOT call dist.barrier() here. When using DeepSpeed launcher,
    # torch.distributed may not be fully initialized yet at this point.
    # Data loading is a local operation - no synchronization needed.
    rank0_print(f"ðŸ”§ Starting data loading...")
    
    dataset_cls = (
        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset
    )

    hidden_data = data_args.hidden_data
    loader_train = None
    
    rank0_print("Loading data metadata...")

    # Handle training data - only modify structure, don't load heavy data
    if data_args.data_path:
        train_json = json.load(open(data_args.data_path, "r"))
        if getattr(data_args, 'max_samples', 0) > 0:
            train_json = train_json[:data_args.max_samples]
            rank0_print(f"âš¡ Quick-test mode: using only {len(train_json)} samples")
        rank0_print(f"Loaded {len(train_json)} training examples from {data_args.data_path}")

    # Collect needed task_ids so HiddenStateLoader only loads what's needed
    needed_ids = None
    max_hs = getattr(data_args, 'max_samples', 0)
    if max_hs > 0 and data_args.data_path:
        needed_ids = [item['id'] for item in train_json if 'id' in item]
        rank0_print(f"Will load hidden states for {len(needed_ids)} task_ids only")

    # Load HiddenStateLoader once in main process.
    # On Linux, fork() uses copy-on-write so workers share the same memory.
    rank0_print("Loading HiddenStateLoader in main process (shared via fork COW)...")
    loader_train = HiddenStateLoader(hidden_data, max_samples=max_hs, task_ids=needed_ids)

    # Modify conversation structure without loading hidden states yet
    if data_args.data_path:
        for item in train_json:
            merged_value = item['conversations'][0]['value'] + '\n' + item['conversations'][2]['value']
            new_first_entry = {'from': 'human', 'value': merged_value}
            new_first_entry['value'] += '\n' + 'Now, you are given a step-by-step plan to complete this task as follow: '
            item['conversations'][0] = new_first_entry
            
            # For non-lazy mode, load hidden states and plans immediately
            if not data_args.lazy_preprocess:
                hidden_state, plan = loader_train.get_hidden_state_and_plan(item['id'])
                hidden_length = hidden_state.shape[0]
                if hidden_length >= prepended_length:
                    hidden_state = hidden_state[:prepended_length, :]
                item['hidden_state'] = hidden_state
                item['plan'] = plan
            else:
                # For lazy mode, only store metadata (id and plan text)
                # Hidden states will be loaded in __getitem__ by worker processes
                if 'id' in item:
                    pass

            # Remove the second entry (original first gpt response)
            del item['conversations'][1:3]

    # === Split validation set from training set if eval_data_path not provided ===
    if data_args.eval_data_path is None:
        rng = random.Random(42)
        rng.shuffle(train_json)
        n_total = len(train_json)
        n_eval = max(1, int(n_total * data_args.eval_ratio))
        eval_json = train_json[:n_eval]
        train_json = train_json[n_eval:]
        rank0_print(f"Split dataset: {len(train_json)} train, {len(eval_json)} eval")
    else:
        eval_json = json.load(open(data_args.eval_data_path, "r"))
        rank0_print(f"Loaded {len(eval_json)} eval examples from {data_args.eval_data_path}")

    # Create datasets with appropriate parameters
    if data_args.lazy_preprocess:
        train_dataset = dataset_cls(
            train_json, 
            tokenizer=tokenizer, 
            model_path=model_path, 
            hidden_state_loader=loader_train,  # Share single loader via fork COW
            prepended_length=prepended_length,
            hidden_data_path=None,  # No need, loader already provided
        )
        eval_dataset = dataset_cls(
            eval_json, 
            tokenizer=tokenizer, 
            model_path=model_path,
            hidden_state_loader=loader_train,  # Share single loader via fork COW
            prepended_length=prepended_length,
            hidden_data_path=None,  # No need, loader already provided
        )
    else:
        train_dataset = dataset_cls(train_json, tokenizer=tokenizer, model_path=model_path)
        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer, model_path=model_path)

    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)
    
    rank0_print("ðŸ”§ Data loading complete")

    return dict(
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator
    )
